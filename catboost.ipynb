{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans\n",
    "from shapely.geometry import Polygon, Point\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import  mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from os.path import join\n",
    "import catboost as cat\n",
    "import shapefile as shp\n",
    "from tqdm import tqdm\n",
    "from vincenty import vincenty\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_shapefile(path, num):\n",
    "\n",
    "    sh = shp.Reader(join(path, 'zaf_admbnda_adm{}_2016SADB_OCHA.shp'.format(num)))\n",
    "    print(len(sh.shapes()))\n",
    "\n",
    "    shapeRecs = sh.shapeRecords()\n",
    "    all_shapes = sh.shapes()\n",
    "    data_ = []\n",
    "    shapes = []\n",
    "    for i in range(len(sh.shapes())-1):\n",
    "        data_.append(shapeRecs[i].record)\n",
    "        shapes.append(all_shapes[i].points)\n",
    "    df = pd.DataFrame.from_records(data_)\n",
    "    fields = sh.fields\n",
    "    df.columns =[f[0] for f in fields[1:]]\n",
    "    df['shape_adm{}'.format(num)] = shapes\n",
    "    df['shape_adm{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:Polygon(x))\n",
    "    df['area_adm{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:x.area)\n",
    "    df['centroid_lon{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:(x.centroid).coords[0][0])\n",
    "    df['centroid_lat{}'.format(num)] = df['shape_adm{}'.format(num)].apply(lambda x:(x.centroid).coords[0][1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"./Train.csv\")\n",
    "test=pd.read_csv(\"./Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.concat([train,test],ignore_index=True)\n",
    "max_1=data[\"total_households\"].max()\n",
    "max_2=data[\"total_individuals\"].max()\n",
    "data[\"total_households\"]/=max_1\n",
    "data[\"total_individuals\"]/=max_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disposal of luxury items\n",
    "luxury_stuff = ['psa_01','car_01','stv_00']\n",
    "not_luxury_stuff = ['psa_00','car_00','stv_01']\n",
    "data['luxury_stuff'] = data[luxury_stuff].sum(axis=1)\n",
    "data['not_luxury_stuff'] = data[not_luxury_stuff].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kmean clusters\n",
    "to_drop=[\"dw_12\",\"dw_13\",\"lan_13\",\"pw_07\",\"pw_08\",'dw_00','dw_02', 'dw_06','psa_02','lan_02','lan_03','lan_04','lan_05','lan_08', 'pw_01' , 'lan_07','NL']\n",
    "data_copy=data.copy()\n",
    "columns=data_copy.drop([\"ward\",\"ADM4_PCODE\",\"target\"]+to_drop,1).columns\n",
    "data_copy=data_copy[columns]\n",
    "km=KMeans(10,random_state=1992)\n",
    "km=km.fit(data_copy[columns])\n",
    "# predict the cluster of each area\n",
    "data[\"cluster\"]=km.predict(data_copy[columns]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimentionality for dwelling features\n",
    "pca = PCA()\n",
    "dwelling_features =  data.filter(regex='dw_.*')\n",
    "df_pca = pca.fit_transform(dwelling_features)\n",
    "\n",
    "data['pca_dw_0'] = df_pca[:,0]\n",
    "data['pca_dw_1'] = df_pca[:,1]\n",
    "# reduce dimentionality for language features\n",
    "pca = PCA()\n",
    "lan_features =  data.filter(regex='lan_.*')\n",
    "df_pca = pca.fit_transform(lan_features)\n",
    "\n",
    "data['pca_lan_0'] = df_pca[:,0]\n",
    "data['pca_lan_1'] = df_pca[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the household size\n",
    "data['Household_Size'] = (data['total_individuals']/data['total_households'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most frequent dwelling type\n",
    "arank = data.filter(regex='dw_.*').apply(np.argsort, axis=1)\n",
    "ranked_lang = pd.DataFrame(data.filter(regex='dw_.*').columns.to_series()[arank.values[:,::-1][:,1:4]])\n",
    "data['dwelling_type_1'] = ranked_lang[0]\n",
    "data['dwelling_type_2'] = ranked_lang[1]\n",
    "data['dwelling_type_3'] = ranked_lang[2]\n",
    "\n",
    "\n",
    "data.loc[data.dwelling_type_1.isin(['dw_07','dw_08']), 'dwelling_type_1'] = 'shack'\n",
    "data.loc[data.dwelling_type_2.isin(['dw_07','dw_08']), 'dwelling_type_2'] = 'shack'\n",
    "data.loc[data.dwelling_type_3.isin(['dw_07','dw_08']), 'dwelling_type_3'] = 'shack'\n",
    "\n",
    "\n",
    "luxury_dwelling_type = ['dw_02', 'dw_03', 'dw_04', 'dw_05', 'dw_06', 'dw_09']\n",
    "data.loc[data.dwelling_type_1.isin(luxury_dwelling_type), 'dwelling_type_1'] = 'luxury'\n",
    "data.loc[data.dwelling_type_2.isin(luxury_dwelling_type), 'dwelling_type_2'] = 'luxury'\n",
    "data.loc[data.dwelling_type_3.isin(luxury_dwelling_type), 'dwelling_type_3'] = 'luxury'\n",
    "data['dw_luxury'] = data[luxury_dwelling_type[0:6]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#water access\n",
    "data['water_access'] = data.filter(regex='pw_.*').idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top points of interests(POIs) of each ADM in specific categories such as 'Facilities', 'Education Facility', etc\n",
    "\n",
    "BASE_URL = \"https://places.sit.ls.hereapi.com/places/v1/\"\n",
    "NEARBY_URL = BASE_URL + \"discover/here?at={},{}&apiKey={}\"\n",
    "SUGGEST_URL = BASE_URL + \"/autosuggest?in={},{};r=300000&q={}'&apiKey={}\"\n",
    "\n",
    "categories = ['hospital or health care facility','Facilities', 'Education Facility','Public Transport',\\\n",
    " 'Government or Community Facility']\n",
    "\n",
    "list_dicts = []\n",
    "for i in tqdm(range(data.shape[0])):\n",
    "    for cat in categories:\n",
    "        resp = requests.get(url=SUGGEST_URL.format(data.loc[i,'lat'], data.loc[i,'lon'], cat,\"h5lCzAFIoFjRPp94SKO1uTIjjcIkaR3h_uWduXUvcRI\" ))\n",
    "        try:\n",
    "            resp = resp.json()\n",
    "            resp['id'] = i\n",
    "            resp['suggested_type'] = cat\n",
    "            list_dicts.append(resp)\n",
    "        except: \n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean distance to each category type\n",
    "for list_ in tqdm(list_dicts): \n",
    "    id_ = list_['id']\n",
    "    try:\n",
    "        results = list_['results']\n",
    "        cat = results[0]['category']\n",
    "        data.loc[id_, 'nb_establishments_type_{}'.format(cat)] = len(results)\n",
    "        distances = []\n",
    "        for i in range(len(results)):\n",
    "            if 'distance' in results[i].keys():\n",
    "                distances.append(results[i]['distance'])\n",
    "        data.loc[id_, 'mean_distance_to_establishments_type_{}'.format(cat)] = np.mean(distances)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4392\n213\n52\n"
    }
   ],
   "source": [
    "df_adm4 = read_shapefile('./zaf_adm_2016SADB_OCHA_SHP/',4)\n",
    "df_adm3 = read_shapefile('./zaf_adm_2016SADB_OCHA_SHP/',3)\n",
    "df_adm2 = read_shapefile('./zaf_adm_2016SADB_OCHA_SHP/',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm4 = df_adm4.merge(df_adm3[['ADM3_PCODE', 'area_adm3','centroid_lon3', 'centroid_lat3']], on=['ADM3_PCODE'], how='left')\n",
    "df_adm4 = df_adm4.merge(df_adm2[['ADM2_PCODE', 'area_adm2','centroid_lon2', 'centroid_lat2']], on=['ADM2_PCODE'], how='left')\n",
    "df = data.copy()\n",
    "df = df.merge(df_adm4.filter(regex='(area.*)|(ADM4_PCODE)|(centroid.*)'), on=['ADM4_PCODE'], how='left')\n",
    "assert(df.shape[0]==data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get ID of ADM4_PCODE\n",
    "data[\"ADM4_PCODE\"]=data[\"ADM4_PCODE\"].str[2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance to center of each ADM\n",
    "data[\"distance_to_center\"] = data.apply(\n",
    "    lambda x: vincenty((x[\"lat\"], x[\"lon\"]), (x[\"centroid_lat3\"], x[\"centroid_lon3\"]),'kilometers'), axis = 1)\n",
    "\n",
    "data[\"distance_to_center2\"] = data.apply(\n",
    "    lambda x: vincenty((x[\"lat\"], x[\"lon\"]), (x[\"centroid_lat2\"], x[\"centroid_lon2\"]),'kilometers'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total_households\"]*=max_1\n",
    "data[\"total_individuals\"]*=max_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[~data.target.isna()]\n",
    "test=data[data.target.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./submissions/'):\n",
    "    os.mkdir('./submissions/')\n",
    "i = len(os.listdir('./submissions'))+1\n",
    "def train_func(train,test,target_name,features,params,num_class,submission_name='sub_'.format(i)):\n",
    "    oof_train = np.zeros((len(train),num_class))\n",
    "    oof_test  = np.zeros((len(test),num_class))\n",
    "    importances=[]\n",
    "    cv_scores = []\n",
    "    train_scores=[]\n",
    "    cat_features = []\n",
    "    dtest=cat.Pool(data=test[features], cat_features=cat_features, feature_names=features)\n",
    "    \n",
    "    for ind, (trn_ind, val_ind) in ( enumerate(kfolds.split(train,train[target_name])) ):\n",
    "\n",
    "        print('fold_n',ind+1, 'started at', time.ctime())\n",
    "        X_train, X_valid = train.loc[trn_ind][features], train.loc[val_ind][features]\n",
    "        y_train, y_valid = train.loc[trn_ind][target_name], train.loc[val_ind][target_name]\n",
    "        dtrain=cat.Pool(data=X_train[features],label=y_train,cat_features=cat_features,\n",
    "                        feature_names=features)\n",
    "        dval=cat.Pool(data=X_valid[features],label=y_valid,cat_features=cat_features,\n",
    "                        feature_names=features)\n",
    " \n",
    "        model = cat.CatBoost(params)\n",
    "    \n",
    "        model.fit(dtrain, eval_set=dval, use_best_model=True)\n",
    "\n",
    "        val_pred = model.predict(dval)\n",
    "        train_pred = model.predict(dtrain,thread_count=4)\n",
    "        test_pred = model.predict(dtest,thread_count=4)\n",
    "\n",
    "              \n",
    "        oof_train[val_ind,:] += np.reshape(val_pred,(-1,num_class))\n",
    "        oof_test += np.reshape(test_pred,(-1,num_class))\n",
    "        score_fold_validation=np.sqrt(mean_squared_error(y_valid, val_pred))\n",
    "        score_fold_train=np.sqrt(mean_squared_error(y_train, train_pred))\n",
    "\n",
    "        train_scores.append(score_fold_train)\n",
    "        cv_scores.append(score_fold_validation)\n",
    "        print('-Train Score: {} - CV Score : {}'.format(score_fold_train, score_fold_validation))\n",
    "    \n",
    "    end_train_score=np.mean(train_scores)\n",
    "    train_scores.append(end_train_score)\n",
    "    \n",
    "    oof_score=np.sqrt(mean_squared_error(train[target_name], oof_train))\n",
    "    cv_scores.append(oof_score)\n",
    "    print(\"\\n\\ntraining is done : train score {} - oof Score {}\".format(str(end_train_score),str(oof_score)))\n",
    "    # create and save the submission\n",
    "    submission=test[[\"ward\"]]\n",
    "    submission[\"target\"]=oof_test/nfolds\n",
    "    # check if the submission folder exists. If not, create it.\n",
    "    submission.to_csv(\"./submissions/{}\".format(submission_name),index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True,inplace=True)\n",
    "num_boost_round=10000000\n",
    "nfolds = 5\n",
    "kfolds = KFold(n_splits=nfolds, shuffle=True, random_state=15468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "48\n"
    }
   ],
   "source": [
    "features = ['ADM4_PCODE', 'car_00', 'car_01', 'dw_01', 'dw_03', 'dw_04', 'dw_05', 'dw_07', 'dw_08', 'dw_09', 'dw_10', 'dw_11', 'lan_00', 'lan_01', 'lan_06', 'lan_09', 'lan_10', 'lan_11', 'lan_12', 'lan_14', 'lat', 'lgt_00', 'lln_00', 'lln_01', 'lon', 'pg_00', 'pg_01', 'pg_02', 'pg_03', 'pg_04', 'psa_00', 'psa_01', 'psa_03', 'psa_04', 'pw_00', 'pw_02', 'pw_03', 'pw_04', 'pw_05', 'pw_06', 'stv_00', 'stv_01', 'total_households', 'total_individuals', 'luxury_stuff', 'not_luxury_stuff', 'cluster','area_adm4']\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "fold_n 1 started at Fri Mar  6 11:03:39 2020\n-Train Score: 1.8773650748270836 - CV Score : 3.283099492709999\nfold_n 2 started at Fri Mar  6 11:03:41 2020\n-Train Score: 1.4829423798019148 - CV Score : 3.1140304727040062\nfold_n 3 started at Fri Mar  6 11:03:44 2020\n-Train Score: 1.8306926360645501 - CV Score : 3.400646793117392\nfold_n 4 started at Fri Mar  6 11:03:47 2020\n-Train Score: 1.304331734453534 - CV Score : 2.9579554665491052\nfold_n 5 started at Fri Mar  6 11:03:51 2020\n-Train Score: 1.7122022944976076 - CV Score : 3.2045925537246283\n\n\ntraining is done : train score 1.6415068239289379 - oof Score 3.195601138969688\n"
    }
   ],
   "source": [
    "# Train Model and predictions\n",
    "params = {\n",
    "    \"learning_rate\": 0.055,\n",
    "    \"random_seed\": 0,\n",
    "    \n",
    "    \"thread_count\": -1,\n",
    "    \"iterations\": 10000000,\n",
    "    \n",
    "    \"loss_function\": \"RMSE\",\n",
    "    \"eval_metric\": \"RMSE\",\n",
    "        \n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"bagging_temperature\": 1,  \n",
    "    \n",
    "    \"depth\": 4,\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 55,\n",
    "\n",
    "    \"verbose_eval\": False,\n",
    "    \"use_best_model\": True,\n",
    "}\n",
    "train_func(train,test,\"target\",features,params=params,num_class=1, submission_name='catboost_surface.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}